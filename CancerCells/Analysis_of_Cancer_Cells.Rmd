---
title: "Hands-On with Unsupervised Learning Analysis of Cancer Cells"
author: "Ashley Lee"
date: "5/1/2018"
output: 
  html_document: 
    keep_md: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Section 1

Preparing the Data 

```{r}
# Complete the following code to input the data and store as wisc.df
wisc.df <- read.csv("WisconsinCancer.csv")
head(wisc.df)
```

```{r}
# Convert the features of the data: wisc.data
wisc.data <- as.matrix(wisc.df[,3:(ncol(wisc.df)-1)])
```

```{r}
# Set the row names of wisc.data
rownames(wisc.data) <- wisc.df$id
head(wisc.data)
```

```{r}
# Create diagnosis vector by completing the missing code
diagnosis <- as.numeric(wisc.df$diagnosis == "M")
diagnosis
```

Exploratory Data Analysis 
> The functions dim(), length(), grep() and sum() may be useful for answering the first 3 questions above.

**Q1.** How many observations are in this dataset?
**A1.** 
```{r}
nrow(wisc.data)
```

**Q2.** How many variables/features in the data are suffixed with _mean?
**A2.** 
```{r}
# Return things with the "mean" in them 
## grep("mean", colnames(wisc.data), value = TRUE)

# Return all things without the "mean" in them (default)
## grep("mean", colnames(wisc.data), value = FALSE) 

length( grep("mean", colnames(wisc.data)) )
```

**Q3.** How many of the observations have a malignant diagnosis?
**A3.** 
```{r}
sum(diagnosis)
```


## Section 2

Performing PCA 

```{r}
# Check column means and standard deviations
colMeans(wisc.data)

apply(wisc.data, 2, sd)
```

```{r}
# Perform PCA on wisc.data by completing the following code
wisc.pr <- prcomp(wisc.data, scale = TRUE)
```

```{r}
# Look at summary of results
summary(wisc.pr)
```

**Q4.** From your results, what proportion of the original variance is captured by the first principal components (PC1)?
**A4.**
44 %

**Q5.** How many principal components (PCs) are required to describe at least 70% of the original variance in the data?
**A5.**
3 PCs

**Q6.** How many principal components (PCs) are required to describe at least 90% of the original variance in the data?
**A6.**
7 PCs

** To get answers, just look at the output of summary(wisc.pr) ** 

Interpreting PCA Results 

```{r}
# Plotting wisc.pr 
biplot(wisc.pr)
```

**Q7.** What stands out to you about this plot? Is it easy or difficult to understand? Why?
**A7.** 
The data points are clustered and it is difficult to see one point of data. It is difficult to understand, as we have too much data for the graph to be clean and easy to interpret. 

```{r}
# Scatter plot observations by components 1 and 2
plot( wisc.pr$x[, 1] , wisc.pr$x[, 2], col = (diagnosis + 1), 
      pch = as.vector(wisc.df$diagnosis), 
      xlab = "PC1", ylab = "PC2")
```

**Q8.** Repeat the same for principal components 1 and 3. What do you notice about these plots?
**A8.**
The data for the malignant cells are plotted in "red", wherease the data for the benign cells are plotted in "black." Looking at the graph, we can see that there is more vairability in the malignant cells than the benigns cells.

```{r}
# Repeat for components 1 and 3
plot(wisc.pr$x[, c(1, 3)], col = (diagnosis + 1), xlab = "PC1", ylab = "PC3")
```


```{r}
# Calculate variance of each component
pr.var <- wisc.pr$sdev^2

# Proportion of variance 
pve <- pr.var / sum(pr.var)

# Cumulative Sum 
cum_pve <- cumsum(pve)
```

```{r}
# Alternative scree plot of the same data, note data driven y-axis
barplot(pve, ylab = "Precent of Variance Explained",
     names.arg=paste0("PC",1:length(pve)), las=2, 
     axes = FALSE)

axis(2, at=pve, labels=round(pve,2)*100 )

```

```{r}
# Plot proportion of variance explained
plot(pve , xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")
```

```{r}
# Plot cumulative proportion of variance explained
plot(cum_pve , xlab = "Principal Component", 
     ylab = "Cumulative Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")
```

Selecting Number of Cluster 

```{r}
# Heirarchical clustering 

## Scale the sic.data data: data.scaled 
data.scaled <- scale(wisc.data)

## Calculate distance matrix needed for hclust 
data.dist <- dist(data.scaled)

## Clustering by hclust()
wisc.hclust <- hclust(data.dist)

## Plot 
plot (wisc.hclust)
```

```{r}
## Cut the tree so that it has 4 clusters
wisc.hclust.cluster <- cutree ( wisc.hclust, k=4 )
table(wisc.hclust.cluster)
```

```{r}
# How many M or 1 (cancer) and B or 0 (non-cancer) are in each cluster? 
table(wisc.hclust.cluster, diagnosis)
```


**Q9.** Can you find a better cluster vs. diagnosis match by cutting into a different number of clusters between 2 and 10?
**A9.** 
```{r}
# SKIP 
```


## Section 4

K-Means Clustering and Comparing Results 

```{r}
# K-means model
wisc.km <- kmeans(scale(wisc.data), 2, nstart=20)
```

```{r}
# Compare the cluster membership of the k-meansmodel to the actual diagnoses
table(wisc.km$cluster, diagnosis)
```

**Q10.** How well does k-means separate the two diagnoses? How does it compare to your hclust results?
**A10.**
```{r}
# Compare hclust and kmeans
table(wisc.hclust.cluster, wisc.km$cluster)
```


## Section 4

Clustering on PCA Results 

```{r}
## Use the distance along the first 7 PCs for clustering i.e. wisc.pr$x[, 1:7]
dist.pr.hclust <- dist(wisc.pr$x[, 1:7])
wisc.pr.hclust <- hclust(dist.pr.hclust)
plot(wisc.pr.hclust)
```

```{r}
# Cut this hierarchical clustering model into 4 clusters
wisc.pr.hclust.clusters <- cutree(wisc.pr.hclust, k = 4)
table (wisc.pr.hclust.clusters)
```

**Q11.** How well does the newly created model with four clusters separate out the two diagnoses?
**A11.**
```{r}
# Compare the results from your new hierarchical clustering model with the actual diagnoses
table (wisc.pr.hclust.clusters, diagnosis)
```

**Q12.** How well do the k-means and hierarchical clustering models you created in previous sections (i.e. before PCA) do in terms of separating the diagnoses? Again, use the table() function to compare the output of each model (wisc.km$cluster and wisc.hclust.clusters) with the vector containing the actual diagnoses.












